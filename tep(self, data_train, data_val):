[1mdiff --git a/naslib/optimizers/oneshot/gsparsity/optimizer.py b/naslib/optimizers/oneshot/gsparsity/optimizer.py[m
[1mindex 32da87bfc..72061fa73 100644[m
[1m--- a/naslib/optimizers/oneshot/gsparsity/optimizer.py[m
[1m+++ b/naslib/optimizers/oneshot/gsparsity/optimizer.py[m
[36m@@ -3,25 +3,28 @@[m [mimport logging[m
 import torch[m
 [m
 from naslib.search_spaces.core.primitives import MixedOp[m
[31m-from naslib.optimizers.oneshot.darts.optimizer import DARTSOptimizer[m
[32m+[m[32mfrom naslib.optimizers.core.metaclasses import MetaOptimizer[m
[32m+[m[32mfrom naslib.utils.utils import count_parameters_in_MB[m
[32m+[m[32mfrom naslib.search_spaces.core.query_metrics import Metric[m
[32m+[m
[32m+[m[32mimport naslib.search_spaces.core.primitives as ops[m
 import ProxSGD_for_groups as ProxSGD[m
 import utils_sparsenas[m
 [m
 logger = logging.getLogger(__name__)[m
 [m
 [m
[31m-class GSparseOptimizer(DARTSOptimizer):[m
[32m+[m[32mclass GSparseOptimizer(MetaOptimizer):[m
     """[m
     Implements Group Sparsity as defined in[m
[31m-[m
[32m+[m[32m        # Add name of authors[m
         GSparsity: Unifying Network Pruning and [m
         Neural Architecture Search by Group Sparsity[m
     """[m
     def __init__([m
         self,[m
         config,[m
[31m-        op_optimizer: torch.optim.Optimizer = torch.optim.SGD,[m
[31m-        arch_optimizer: torch.optim.Optimizer = torch.optim.Adam,[m
[32m+[m[32m        op_optimizer: torch.optim.Optimizer = torch.optim.SGD,[m[41m        [m
         loss_criteria=torch.nn.CrossEntropyLoss(),[m
     ):[m
         """[m
[36m@@ -31,16 +34,20 @@[m [mclass GSparseOptimizer(DARTSOptimizer):[m
             epochs (int): Number of epochs. Required for tau[m
             mu (float): corresponds to the Weight decay[m
             threshold (float): threshold of pruning[m
[31m-            op_optimizer (torch.optim.Optimizer): optimizer for the op weights[m
[31m-            arch_optimizer (torch.optim.Optimizer): optimizer for the architecture weights[m
[32m+[m[32m            op_optimizer (torch.optim.Optimizer): optimizer for the op weights[m[41m            [m
             loss_criteria: The loss.[m
             grad_clip (float): Clipping of the gradients. Default None.[m
         """[m
[31m-        super().__init__(config, op_optimizer, arch_optimizer, loss_criteria)[m
[32m+[m[32m        super(GSparseOptimizer, self).__init__()[m
 [m
[32m+[m[32m        self.config = config[m
[32m+[m[32m        self.op_optimizer = op_optimizer[m
[32m+[m[32m        self.loss = loss_criteria[m
[32m+[m[32m        self.dataset = config.dataset[m
         self.grad_clip = config.search.grad_clip[m
         self.threshold = config.search.threshold[m
         self.mu = config.search.mu[m
[32m+[m[32m        self.device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")[m
 [m
     @staticmethod[m
     def update_ops(edge):[m
[36m@@ -50,6 +57,157 @@[m [mclass GSparseOptimizer(DARTSOptimizer):[m
         """[m
         primitives = edge.data.op[m
         edge.data.set("op", GSparseMixedOp(primitives))[m
[32m+[m[41m    [m
[32m+[m[32m    @staticmethod[m
[32m+[m[32m    def add_alphas(edge):[m
[32m+[m[32m        """[m
[32m+[m[32m        Function to add the pruning flag 'alpha' to the edges.[m
[32m+[m[32m        And initialize a group name for all primitives.[m
[32m+[m[32m        """[m
[32m+[m[32m        len_primitives = len(edge.data.op)[m
[32m+[m[32m        alpha = torch.nn.Parameter([m
[32m+[m[32m           torch.ones(size=[len_primitives], requires_grad=False)[m
[32m+[m[32m        )[m
[32m+[m[32m        group = torch.nn.Parameter([m
[32m+[m[32m           torch.zeros(size=[len_primitives], requires_grad=False)[m
[32m+[m[32m        )[m
[32m+[m[32m        edge.data.set("alpha", alpha, shared=True)[m
[32m+[m[32m        edge.data.set("group", group, shared=True)[m
[32m+[m[41m    [m
[32m+[m[32m    def group_primitives(graph):[m
[32m+[m[32m        """[m
[32m+[m[32m        Function to group similar operations together[m
[32m+[m[32m        A group could be the same operation in all cells of the same type (normal or reduce).[m[41m [m
[32m+[m[32m        For example, one group is op 3 of edge 7 in Cells 0-1/3-4/5-7 (normal cells),[m[41m [m
[32m+[m[32m        another group is op 3 of edge 7 in Cell 2/5 (reduce cells).[m
[32m+[m[41m    [m
[32m+[m[32m        A group could also be the same operation in all cells in a stage (there may be multiple stages).[m[41m [m
[32m+[m[32m        For example, one group is op 3 of edge 7 in all cells of stage_normal_1 (Cells 0-1),[m[41m [m
[32m+[m[32m        another group is op 3 of edge 7 in all cells of stage_reduce_1 (Cell 2),[m[41m [m
[32m+[m[32m        another group is op 3 of edge 7 in all cells of stage_normal_2 (Cells 3-4),[m[41m [m
[32m+[m[32m        another group is op 3 of edge 7 in all cells of stage_reduce_2 (Cell 5),[m[41m [m
[32m+[m[32m        one group is op 3 of edge 7 in all cells of stage_normal_3 (Cells 6-7).[m
[32m+[m[41m    [m
[32m+[m[32m        """[m
[32m+[m[32m        #makrograph-subgraph_at(4).normal_cell-edge(1,6).primitive-6.op.1.weight'[m
[32m+[m[32m        #graph.nodes[5]['subgraph'].edges[1, 6]['op'].primitives[6].op[m
[32m+[m[32m        #graph.nodes[5]['subgraph'].edges[1, 6][m
[32m+[m[32m        #graph.nodes[5]['subgraph'].scope[m
[32m+[m
[32m+[m[32m    def step(self, data_train, data_val):[m
[32m+[m[32m        """[m
[32m+[m[32m        Run one optimizer step with the batch of training and test data.[m
[32m+[m
[32m+[m[32m        Args:[m
[32m+[m[32m            data_train (tuple(Tensor, Tensor)): A tuple of input and target[m
[32m+[m[32m                tensors from the training split[m
[32m+[m[32m            data_val (tuple(Tensor, Tensor)): A tuple of input and target[m
[32m+[m[32m                tensors from the validation split[m
[32m+[m[32m            error_dict[m
[32m+[m
[32m+[m[32m        Returns:[m
[32m+[m[32m            dict: A dict containing training statistics (TODO)[m
[32m+[m[32m        """[m
[32m+[m[32m        if self.using_step_function:[m
[32m+[m[32m            raise NotImplementedError()[m
[32m+[m
[32m+[m[32m    def train_statistics(self):[m
[32m+[m[32m        """[m
[32m+[m[32m        If the step function is not used we need the statistics from[m
[32m+[m[32m        the optimizer[m
[32m+[m[32m        """[m
[32m+[m[32m        if not self.using_step_function:[m
[32m+[m[32m            raise NotImplementedError()[m
[32m+[m
[32m+[m[32m    def test_statistics(self):[m
[32m+[m[32m        """[m
[32m+[m[32m        Return anytime test statistics if provided by the optimizer[m
[32m+[m[32m        """[m
[32m+[m[32m        pass[m
[32m+[m
[32m+[m[32m    @abstractmethod[m
[32m+[m[32m    def adapt_search_space(self, search_space, scope=None):[m
[32m+[m[32m        """[m
[32m+[m[32m        Modify the search space to fit the optimizer's needs,[m
[32m+[m[32m        e.g. discretize, add architectural parameters, ...[m
[32m+[m
[32m+[m[32m        To modify the search space use `search_space.update(...)`[m
[32m+[m
[32m+[m[32m        Good practice is to deepcopy the search space, store[m
[32m+[m[32m        the modified version and leave the original search space[m
[32m+[m[32m        untouched in case it is beeing used somewhere else.[m
[32m+[m
[32m+[m[32m        Args:[m
[32m+[m[32m            search_space (Graph): The search space we are doing NAS in.[m
[32m+[m[32m            scope (str or list(str)): The scope of the search space which[m
[32m+[m[32m                should be optimized by the optimizer.[m
[32m+[m[32m        """[m
[32m+[m[32m        raise NotImplementedError()[m
[32m+[m
[32m+[m[32m    def new_epoch(self, epoch):[m
[32m+[m[32m        """[m
[32m+[m[32m        Function called at the beginning of each new search epoch. To be[m
[32m+[m[32m        used as hook for the optimizer.[m
[32m+[m
[32m+[m[32m        Args:[m
[32m+[m[32m            epoch (int): Number of the epoch to start.[m
[32m+[m[32m        """[m
[32m+[m[32m        pass[m
[32m+[m
[32m+[m[32m    def before_training(self):[m
[32m+[m[32m        """[m
[32m+[m[32m        Function called right before training starts. To be used as hook[m
[32m+[m[32m        for the optimizer.[m
[32m+[m[32m        """[m
[32m+[m[32m        pass[m
[32m+[m
[32m+[m[32m    def after_training(self):[m
[32m+[m[32m        """[m
[32m+[m[32m        Function called right after training finished. To be used as hook[m
[32m+[m[32m        for the optimizer.[m
[32m+[m[32m        """[m
[32m+[m[32m        pass[m
[32m+[m
[32m+[m[32m    @abstractmethod[m
[32m+[m[32m    def get_final_architecture(self):[m
[32m+[m[32m        """[m
[32m+[m[32m        Returns the final discretized architecture.[m
[32m+[m
[32m+[m[32m        Returns:[m
[32m+[m[32m            Graph: The final architecture.[m
[32m+[m[32m        """[m
[32m+[m[32m        raise NotImplementedError()[m
[32m+[m
[32m+[m[32m    @abstractmethod[m
[32m+[m[32m    def get_op_optimizer(self):[m
[32m+[m[32m        """[m
[32m+[m[32m        This is required for the final validation when[m
[32m+[m[32m        training from scratch.[m
[32m+[m
[32m+[m[32m        Returns:[m
[32m+[m[32m            (torch.optim.Optimizer): The optimizer used for the op weights update.[m
[32m+[m[32m        """[m
[32m+[m
[32m+[m[32m    def get_model_size(self):[m
[32m+[m[32m        """[m
[32m+[m[32m        Returns the size of the model parameters in mb, e.g. by using[m
[32m+[m[32m        `utils.count_parameters_in_MB()`.[m
[32m+[m
[32m+[m[32m        This is only used for logging purposes.[m
[32m+[m[32m        """[m
[32m+[m[32m        return 0[m
[32m+[m
[32m+[m[32m    def get_checkpointables(self):[m
[32m+[m[32m        """[m
[32m+[m[32m        Return all objects that should be saved in a checkpoint during training.[m
[32m+[m
[32m+[m[32m        Will be called after `before_training` and must include key "model".[m
[32m+[m
[32m+[m[32m        Returns:[m
[32m+[m[32m            (dict): with name as key and object as value. e.g. graph, arch weights, optimizers, ...[m
[32m+[m[32m        """[m
[32m+[m[32m        pass[m
[32m+[m
 [m
 [m
 [m
[36m@@ -70,7 +228,6 @@[m [mclass GSparseMixedOp(MixedOp):[m
         before forwarding `x` through the graph as in DARTS[m
         """[m
         # sampled_arch_weight = edge_data.sampled_arch_weight[m
[31m-        # result1 = sum(w * op(x, None) for w, op in zip(sampled_arch_weight, self.primitives))[m
 [m
         summed = sum(op(x, None) for op in self.primitives)[m
 [m
[1mdiff --git a/naslib/search_spaces/darts/conversions.py b/naslib/search_spaces/darts/conversions.py[m
[1mindex 7445192a5..767481e6d 100644[m
[1m--- a/naslib/search_spaces/darts/conversions.py[m
[1m+++ b/naslib/search_spaces/darts/conversions.py[m
[36m@@ -29,6 +29,7 @@[m [mdef convert_naslib_to_genotype(naslib_object):[m
     """convert the naslib representation to Genotype"""[m
     ops_to_genotype = {[m
         "Identity": "skip_connect",[m
[32m+[m[32m        #"DARTSMixedOp": "DARTSMixedOp",[m
         "FactorizedReduce": "skip_connect",[m
         "SepConv3x3": "sep_conv_3x3",[m
         "DilConv3x3": "dil_conv_3x3",[m
